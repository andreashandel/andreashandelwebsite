{
  "hash": "788d4bddbb7f68c6daa225114aa8dbbe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bayesian analysis of longitudinal multilevel data - part 5  \ndescription: Part 5 of a tutorial showing how to fit directly with Stan and cmdstanr.\nauthor: Andreas Handel\ndate: 2024-02-15\ndate-modified: 2025-03-05\naliases: \n  - ../longitudinal-multilevel-bayesian-analysis-5/\ncategories: \n  - R\n  - Data Analysis\n  - Bayesian\n  - Stan\nimage: \"featured.png\"\nimage-alt: \"Density plot of priors and posteriors for several model parameters.\"\nexecute:\n  echo: true\nengine: knitr\n---\n\n\n\n\n\n\n# Overview\n\nThis is a re-implementation of a prior model using `cmdstanr` and Stan code. It is a continuation of a prior series of posts. You should [start at the beginning](/posts/2022-02-22-longitudinal-multilevel-bayes-1/). \n\nHere is [the Stan code for this example](stancode-2par.stan) and this is [the R script that runs everything](cmdstanr-2par-script.R).\n\n\n# Introduction\n\nA while ago, I wrote [a series of tutorials](/posts/2022-02-22-longitudinal-multilevel-bayes-1/) that discuss fitting longitudinal data using Bayesian multilevel/hierarchical/mixed-effects models.\n\nFor a research project, I now want to implement a model that uses a set of ordinary differential equations (ODEs). I figured to understand what I'm trying to do, I should first teach myself and write it up in a tutorial. \n\nTo implement ODEs with Stan, one can't fully use the `rethinking` or `brms` package, one needs to write at least some Stan code. Based on my needs, it is best if I fully implement the model in Stan and call it from R through `cmdstanr`.\n\nI was going to do all at once, but then realized it's better if I first re-implement the old (non ODE-based) setup with Stan code, and then once that's up and running, switch to the ODE model. \n\nSo this post is an intermediary step to my final goal. It might be of interest to folks to see how to implement this question fully with Stan, even if they don't plan on using ODEs.\n\n\n# Quick recap\n\nI assume you read through the previous posts, at least [part 1](/posts/2022-02-22-longitudinal-multilevel-bayes-1/) which describes the overall setup and the models to be explored, and [part 2](/posts/2022-02-23-longitudinal-multilevel-bayes-2/) which explains the models further and fits the data using `rethinking`. If you didn't, the following won't make much sense üòÅ.\n\nPreviously, I explored several model variants. Here, I'm focusing on the adaptive pooling model (which I previously labeled model 4). As a repeat, here are the model equations.\nI changed the distributions for a few of the parameters since it turned out the ones I used previously made it tricky to sample the priors with Stan and led to convergence issues.\n\n\n$$\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{main model describing the virus trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{Exponential}(1)  \\\\\na_1 \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1 \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\n\\\\\n\\textrm{individal-level priors} \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_b) \\\\\n\\\\\n\\textrm{hyper priors} \\\\\n\\mu_a  \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b  \\sim \\mathrm{Normal}(0.5, 1) \\\\\n\\sigma_a  \\sim \\mathrm{Exponential}(1)  \\\\\n\\sigma_b  \\sim \\mathrm{Exponential}(1)  \n\\end{aligned}\n$$\n\n\n# Model implementation\n\nI previously used the `brms` and `rethinking` R packages to run our `Stan` models in `R`, without having to write `Stan` code. Of course, we could implement the model above in either of those packages. But in preparation of what I really want to do (using ODE models, and eventually fully account for censored data), I need to switch to coding the model in `Stan`. There might be hacks to do it with `brms` or `rethinking`, but it seems more flexible and also more transparent to just code the full model in `Stan`. We'll still run it through R using `cmdstanr`. Links to the Stan and R code files are given at the top of this document.\n\nWe start by loading packages and defining a few settings.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"here\") # for file loading\nlibrary(\"dplyr\") # for data manipulation\nlibrary(\"ggplot2\") # for plotting\nlibrary(\"fs\") # for file path\nlibrary(\"cmdstanr\") # for model fitting\nlibrary(\"bayesplot\") # for plotting results\nlibrary(\"posterior\") # for post-processing\nlibrary(\"loo\") # for model diagnostics\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n############################################\n# some general definitons and setup stuff\n############################################\n#setting random number seed for reproducibility\nrngseed = 1234\n\n# I'll be saving results so we can use them without always running the model\n# Note that the files are often too large for standard Git/GitHub - where this project lives\n# Git Large File Storage should be able to handle it\n# I'm using a simple hack so I don't have to set up Git LFS\n# I am saving these large file to a folder that is synced with Dropbox\n# adjust accordingly for your setup\nfilepath = fs::path(\"C:\",\"Data\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\")\n#filepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\")\nfilename = \"cmdstanr2par.Rds\"\nstanfile <- here('posts','2024-02-15-longitudinal-multilevel-bayes-5',\"stancode-2par.stan\")\n```\n:::\n\n\n\n\n\nWe'll use the same data as before. We just need to reshape it a bit to get it into the format that `cmdstanr` requires.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adjust as necessary\nsimdatloc <- here::here(\"posts\", \"2022-02-22-longitudinal-multilevel-bayes-1\", \"simdat.Rds\")\nsimdat <- readRDS(simdatloc)\n# again using dataset 3 for fitting\n# some formatting to get it into the shape needed for cmdstanr\nNind <- length(unique(simdat$m3$id)) # number of individuals\nNobs <- length(simdat$m3$id) # total number of observations\nfitdat <- list(\n  id = simdat[[3]]$id,\n  outcome = simdat[[3]]$outcome,\n  time = simdat[[3]]$time,\n  dose_adj = simdat[[3]]$dose_adj,\n  Nobs = Nobs,\n  Nind = Nind\n)\n```\n:::\n\n\n\n\n\n# Stan code\n\nWe need to write the Stan model code. While one could embed Stan code inside an R script, I find it best to write that code in a separate file and then load it. Here, the code is in file called `stancode-2par.stan`. This is how it looks like. \n\n```{.stan include=\"stancode-2par.stan\"}\n```\n\nI added some comments to the Stan model, but if you have never written Stan code, this is likely not fully clear. I won't try to explain Stan code in detail here. There are lots of good resources on the [Stan website](https://mc-stan.org/) and other places online. \nNote that the `generated quantities` block is not technically part of the model, and we would get the same results without it. But it is included to compute priors, likelihood and posterior predictions, so we can explore those later. You'll see that used below.\nWhile we could have all the information that's inside the `transformed parameters` block part of the `model` block, that would mean that the `generated quantities` code bits can't access that information. Therefore, all of these intermediate steps are done in `transformed parameters` and then used in both the `model` and the `generated quantities` block. I generally find it easiest to have any distributional/probabilistic bits of code (those that include the `~` sign) in the `model` block, and everything else as much as possible in `transformed parameters`.\n\n\nThis uses `cmdstanr` to load and compile the Stan model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make Stan model. \nstanmod1 <- cmdstanr::cmdstan_model(stanfile, \n                                    pedantic=TRUE, \n                                    force_recompile=TRUE)\n```\n:::\n\n\n\n\n\n\n# Model fitting settings\n\nTo fully specify a model, we need to define the details of the model run (e.g., the random seed, the number of warm-up and sampling steps). This often requires some tuning to find the right values. It is generally a good idea to start with fewer iterations and less stringent fitting criteria while debugging the code. Once everything seems working, one can do one long final run. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# settings for fitting\n# keeping values somewhat low to make things run reasonably fast\n# for 'production' you would probably want to sample more\n# and set more stringent conditions\nfs_m1 <- list(\n  warmup = 1500,\n  sampling = 2000,\n  max_td = 18, # tree depth\n  adapt_delta = 0.9999,\n  chains = 5,\n  cores = 5,\n  seed = rngseed,\n  save_warmup = TRUE\n)\n```\n:::\n\n\n\n\n\nIt is also a good idea to specify starting values for the parameters. Models can be run without providing starting values (also called initial conditions). In that case, `cmdstanr` will pick default values. However, setting starting values can often improve convergence and thus cut down on required computing time. It also requires one to think a bit more carefully about their model, which is a good thing üòÅ.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# separate definition of initial values, added to fs_m1 structure\n# a different sample will be drawn for each chain\n# there's probably a better way to do that than a for loop\nset.seed(rngseed) #make inits reproducible\ninit_vals_1chain <- function() {\n  (list(\n    mu_a = runif(1, 1, 3),\n    mu_b = runif(1, 0, 1),\n    sigma_a = runif(1, 1, 10),\n    sigma_b = runif(1, 1, 10),\n    a1 = rnorm(1, -0.2, 0.2),\n    b1 = rnorm(1, -0.2, 0.2),\n    sigma = runif(1, 1, 10)\n  ))\n}\ninits <- NULL\nfor (n in 1:fs_m1$chains)\n{\n  inits[[n]] <- init_vals_1chain()\n}\nfs_m1$init <- inits\n```\n:::\n\n\n\n\n\n# Model fitting \n\nAt this point, we have specified everything necessary to run the model. This runs the model with the specified settings. I'm supressing the output but it's useful to look at it when you run it to make sure the sampler is running ok.\n\nThis runs the model. It's not actually run here to speed up generation of this Quarto file, but the code chunk is in the R script, so you can run it.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_m1 <- stanmod1$sample(\n  data = fitdat,\n  chains = fs_m1$chains,\n  init = fs_m1$init,\n  seed = fs_m1$seed,\n  parallel_chains = fs_m1$chains,\n  iter_warmup = fs_m1$warmup,\n  iter_sampling = fs_m1$sampling,\n  save_warmup = fs_m1$save_warmup,\n  max_treedepth = fs_m1$max_td,\n  adapt_delta = fs_m1$adapt_delta,\n  output_dir = filepath\n  \n)\n```\n:::\n\n\n\n\n\n# Model result loading\n\nTo save time, we don't run the model each time, instead we save the results and load them.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# loading previously saved fit.\n# useful if we don't want to re-fit\n# every time we want to explore the results.\n# since the file is too large for GitHub\n# it is stored in a local cloud-synced folder\n# adjust accordingly for your setup\nres_m1 <- readRDS(fs::path(filepath,filename))\n```\n:::\n\n\n\n\n\n\n# Model diagnostics\n\nFirst, we look at diagnostics from the fitting routine to make sure nothing obviously wrong shows up.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(res_m1$cmdstan_diagnose())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-1-08400a.csv not found\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-2-08400a.csv not found\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-3-08400a.csv not found\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-4-08400a.csv not found\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-5-08400a.csv not found\nNo valid input files, exiting.\n$status\n[1] 0\n\n$stdout\n[1] \"File D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-1-08400a.csv not found\\r\\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-2-08400a.csv not found\\r\\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-3-08400a.csv not found\\r\\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-4-08400a.csv not found\\r\\nFile D:/Dropbox/datafiles/longitudinalbayes/stancode-2par-202402271800-5-08400a.csv not found\\r\\nNo valid input files, exiting.\\r\\n\"\n\n$stderr\n[1] \"\"\n\n$timeout\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\nThings look reasonably good, no obvious problems.\n\n\nAnother important check are to make a few diagnostic plots. We'll first need to get the samples, both with and without warmups, to be able to make various figures.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this uses the posterior package to get draws\nsamp_m1 <- res_m1$draws(inc_warmup = FALSE, format = \"draws_df\")\nallsamp_m1 <- res_m1$draws(inc_warmup = TRUE, format = \"draws_df\")\n```\n:::\n\n\n\n\nNow we can look at a few figures. Here I'm showing a trace plot  and a pairs plot. I'm not discussing the plots in detail, you can look up the help file for each R command to learn more.\n\nNote that I'm including some priors here. That's not too meaningful since the prior distributions do not change as the fitting proceeds. Still, it won't hurt to see them. If for some reason the trace plots for the priors look strange (e.g., indicating poor mixing), it means something in the code is wrong.  Similarly, none of the priors should show correlations with each other in the pairs plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# only main parameters \n# excluding parameters that we have for each individual, is too much\nplotpars <- c(\"a1\", \"b1\", \"a1_prior\", \"b1_prior\", \"sigma\")\nbayesplot::color_scheme_set(\"viridis\")\nbp1 <- bayesplot::mcmc_trace(samp_m1, pars = plotpars)\nbp2 <- bayesplot::mcmc_pairs(samp_m1, pars = plotpars)\nplot(bp1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_par_m1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(bp2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_par_m1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# just to get a picture that can be shown together with the post\n# ggsave(\"featured.png\",bp2)\n```\n:::\n\n\n\n\nThe plots look reasonable. Well-mixing chains and no noticeable correlations among parameters.\n\n# Model results\n\nNow that we think we can somewhat trust that the sampling worked, we'll take a look at a summary table for the distributions of some of the model parameters. I cut it off since there are too many to show (each individual as multiple parameters with associated distributions). We'll also look at the posteriors in a graph.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(res_m1$summary(), 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 √ó 10\n   variable     mean   median     sd     mad       q5      q95  rhat ess_bulk\n   <chr>       <dbl>    <dbl>  <dbl>   <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n 1 lp__     -67.7    -67.4    5.71   5.64    -77.6    -58.9     1.00    3167.\n 2 sigma      1.06     1.06   0.0515 0.0515    0.982    1.15    1.00   10131.\n 3 a1         0.0863   0.0863 0.0103 0.00982   0.0689   0.103   1.00    1244.\n 4 b1        -0.107   -0.107  0.0131 0.0129   -0.128   -0.0850  1.00    1443.\n 5 a0[1]      3.18     3.18   0.0278 0.0273    3.13     3.22    1.00    1716.\n 6 a0[2]      3.14     3.14   0.0277 0.0271    3.09     3.18    1.00    1715.\n 7 a0[3]      2.94     2.94   0.0294 0.0287    2.89     2.99    1.00    2024.\n 8 a0[4]      2.88     2.88   0.0305 0.0300    2.83     2.93    1.00    2088.\n 9 a0[5]      2.92     2.92   0.0299 0.0290    2.87     2.97    1.00    2045.\n10 a0[6]      3.01     3.01   0.0288 0.0282    2.96     3.06    1.00    1770.\n11 a0[7]      2.96     2.96   0.0296 0.0293    2.91     3.01    1.00    2007.\n12 a0[8]      2.98     2.98   0.0148 0.0147    2.96     3.01    1.00    7164.\n13 a0[9]      2.91     2.91   0.0162 0.0158    2.88     2.94    1.00    7971.\n14 a0[10]     2.98     2.98   0.0151 0.0149    2.96     3.01    1.00    7333.\n15 a0[11]     2.94     2.94   0.0153 0.0153    2.91     2.96    1.00    7326.\n# ‚Ñπ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\nbp3 <- bayesplot::mcmc_dens_overlay(samp_m1, pars = plotpars)\nplot(bp3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/results_m1-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe estimates for the parameters are fairly close to those used to simulate the data and obtained [previously](/posts/2022-02-23-longitudinal-multilevel-bayes-2/#models-4-and-4a). That's reassuring, since we just re-fit the same model[^priors] using a different R package, we really want to get the same results. \n\n[^priors]: We'll, it's almost the same model, as I mentioned above some of the prior distributions got changed, but that did fortunately not impact our posterior estimates by much.\n\n\n\n# Priors and Posteriors\n\nNext, we'll compare prior and posterior distributions. This can give an indication if the priors were selected well or are too broad or overly influential. To be able to show priors, we needed all that extra information in the _generated quantities_ block in the Stan code. \nThe individual-level posteriors for $a_0[i]$ and $b_0[i]$ are omitted since there are too many. With a bit more data wrangling, one could plot the averages for these parameters, but I couldn't quickly think of how to do it, so I'm skipping it üòÅ. The priors for these parameters are shown since they same for each individual. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data manipulation to get in shape for plotting\npostdf <- samp_m1 %>%\n  select(!ends_with(\"prior\")) %>%\n  select(!starts_with(\".\")) %>%\n  select(-\"lp__\") %>%\n  select(!contains(\"[\"))\npriordf <- samp_m1 %>%\n  select(ends_with(\"prior\")) %>%\n  rename_with(~ gsub(\"_prior\", \"\", .x, fixed = TRUE))\npostlong <- tidyr::pivot_longer(data = postdf, cols = everything(), names_to = \"parname\", values_to = \"value\") %>% mutate(type = \"posterior\")\npriorlong <- tidyr::pivot_longer(data = priordf, cols = everything(), names_to = \"parname\", values_to = \"value\") %>% mutate(type = \"prior\")\nppdf <- dplyr::bind_rows(postlong, priorlong)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1_p1 <- ppdf %>%\n  ggplot() +\n  geom_density(aes(x = value, color = type), linewidth = 1) +\n  facet_wrap(\"parname\", scales = \"free\") +\n  theme_minimal()\nplot(m1_p1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/prior_post_m1-1.png){width=672}\n:::\n:::\n\n\n\n\nThe plots show that the priors don't overly influence the results, the data dominates the posteriors (they move and get more peaked, an indication that the data controls the posterior shape).\n\n\n# Observed versus predicted\n\nAnother useful plot is to look at observed versus predicted results. This is shown in the following plot. The data (black line, $y$ variable) and the model (thin green line, $y_{rep}$) are following each other fairly closely. That's a good sign. Systematic deviations would indicate that the model didn't fully capture the patterns found in the data and might need modification.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred_df <- samp_m1 %>% select(starts_with(\"ypred\"))\nm1_p2 <- bayesplot::ppc_dens_overlay(fitdat$outcome, as.matrix(ypred_df))\nplot(m1_p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/obs_pred_m1-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# Cross-validation tests\n\nWe can explore further doing cross-validation with the `loo` package. For this to work, the Stan code needs to include computation of the log-likelihood (stored in a variable called `log_lik`). We included that in the `Stan` code for this model.\n\nHere are the diagnostics we get from `loo`. I won't go into the details of cross-validation and LOO here, see the [`loo` package website](https://mc-stan.org/loo/) for good explanations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# uses loo package\nloo_m1 <- res_m1$loo(cores = fs_m1$chains, save_psis = TRUE)\nprint(loo_m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 10000 by 264 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -419.5 12.1\np_loo        46.4  4.9\nlooic       839.0 24.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.8]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     259   98.1%   319     \n   (0.7, 1]   (bad)        5    1.9%   <NA>    \n   (1, Inf)   (very bad)   0    0.0%   <NA>    \nSee help('pareto-k-diagnostic') for details.\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(loo_m1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/loo_m1_part1-1.png){width=672}\n:::\n:::\n\n\n\n\nSome values aren't too great. But it's only a few and it's very likely that if we were to run the model with more iterations and more stringent settings, we'll get even better results. You can give it a try üòÅ. \n\nHere's some more LOO diagnostics.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred_df <- samp_m1 %>% select(starts_with(\"ypred\"))\nm1_p3 <- bayesplot::ppc_loo_pit_overlay(\n  y = fitdat$outcome,\n  yrep = as.matrix(ypred_df),\n  lw = weights(loo_m1$psis_object)\n)\nplot(m1_p3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/loo_m1_part2-1.png){width=672}\n:::\n:::\n\n\n\n\nThe marginal posterior predictive plot suggests some improvement might be possible (so that the solid line is more on top of the green lines). [See here for more](https://mc-stan.org/loo/articles/loo2-example.html).\n\n\n# Model predictions\n\nFinally, we want to look at the actual data and the model predictions. \nThese lines of code compute the predictions for both the deterministic part of the model only (the mean) and for individual predictions that account for additional variability around the mean.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# averages and CI for\n# estimates of deterministic model trajectory\n# for each observation\n# this is computed in the transformed parameters block of the Stan code\nmu <- samp_m1 |>\n  select(starts_with(\"virus_pred\")) |>\n  apply(2, quantile, c(0.05, 0.5, 0.95)) |>\n  t()\nrownames(mu) <- NULL\n\n# estimate and CI for prediction intervals\n# the predictions factor in additional uncertainty around the mean (mu)\n# as indicated by sigma\n# this is computed in the predicted-quantities block of the Stan code\n# the average of mu and preds should be more or less the same\n# but preds will have wider uncertainty due to the residual variation sigma\npreds <- samp_m1 |>\n  select(starts_with(\"ypred\")) |>\n  apply(2, quantile, c(0.05, 0.5, 0.95)) |>\n  t()\nrownames(preds) <- NULL\n```\n:::\n\n\n\n\nThis plots the data and the predictions, similarly to previous posts of this series. \nAs before, it shows the deterministic mean and credible interval (I chose 95% here) and the prediction intervals (the very light shaded areas, also chosen here at 95%). Agreement of model with data is good, as before.\n\nNote that previously, I used new data to predict at more time points to get smoother curves. That was easy to do with `rethinking` and `cmdstanr`. I couldn't figure out an easy way to do it here. It is possible to re-run a version of the Stan model that only includes the `generated quantities` block and give it new \"data\" with more time points and thereby return predictions. It should also be possible to do this with R code by recomputing trajectories for more data given the parameter distributions. I couldn't right away think of a good way of doing it, so left it as is. One can still reasonably well compare models and data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# change dose so it looks nicer in plot\ndose <- as.factor(fitdat$dose_adj)\nlevels(dose)[1] <- \"low\"\nlevels(dose)[2] <- \"medium\"\nlevels(dose)[3] <- \"high\"\n\n# place everything into a data frame\nfitpred <- data.frame(\n  id = as.factor(fitdat$id),\n  dose = dose,\n  time = fitdat$time,\n  Outcome = fitdat$outcome,\n  Estimate = mu[, 2],\n  Qmulo = mu[, 1], Qmuhi = mu[, 3],\n  Qsimlo = preds[, 1], Qsimhi = preds[, 3]\n)\n\n# make the plot\npredplot <- ggplot(data = fitpred, aes(x = time, y = Estimate, group = id, color = dose)) +\n  geom_line() +\n  geom_ribbon(aes(x = time, ymin = Qmulo, ymax = Qmuhi, fill = dose, color = NULL), alpha = 0.3, show.legend = F) +\n  geom_ribbon(aes(x = time, ymin = Qsimlo, ymax = Qsimhi, fill = dose, color = NULL), alpha = 0.1, show.legend = F) +\n  geom_point(aes(x = time, y = Outcome, group = id, color = dose), shape = 1, size = 2, stroke = 2) +\n  scale_y_continuous(limits = c(-30, 50)) +\n  labs(\n    y = \"Virus load\",\n    x = \"days post infection\"\n  ) +\n  theme_minimal()\nplot(predplot)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_predictions-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n# Summary and continuation\n\nThis completes the `cmdstanr`/`Stan` re-implementation and exploration of one of the previously explored models. Comparing the results to those found previously, we find good agreement. That's somewhat comforting.\n\nWhile I had planned to now implement the ODE model as a next step, I ended up deciding on one more intermediate step. Namely, I know that the ODE model I want to use has 4 main parameters, as opposed to the 2 parameters used here. I figured I might first build a more complex, 4-parameter non-ODE model before I switch. The obvious candidate for that 4-parameter model is the equation I [mentioned previously](/posts/2022-02-25-longitudinal-multilevel-bayes-4/#alternative-model-for-time-series-trajectory) and that we ended up using in [our paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10310361/).\n\nSo [the next part in this series](/posts/2024-02-16-longitudinal-multilevel-bayes-6/) is a more complex non-ODE model. \n\n<!-- I recommend you go there and at least skim through it. Nothing much new is happening, but it sets the stage for what's coming after. Or, if you are impatient, [jump straight to the ODE model implementation](/posts/2024-02-17-longitudinal-multilevel-bayes-7/). -->\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}