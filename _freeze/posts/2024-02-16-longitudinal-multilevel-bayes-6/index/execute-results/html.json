{
  "hash": "bdfc13f4014936e0c60a5ad04c1dd9fa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bayesian analysis of longitudinal multilevel data - part 6  \ndescription: Part 6 of a tutorial showing how to fit directly with Stan and cmdstanr.\nauthor: Andreas Handel\ndate: 2024-02-16\ndate-modified: last-modified\naliases: \n  - ../longitudinal-multilevel-bayesian-analysis-6/\ncategories: \n  - R\n  - Data Analysis\n  - Bayesian\n  - Stan\nimage: \"featured.png\"\nimage-alt: \"Traceplot for several model parameters.\"\nexecute:\n  echo: true\nengine: knitr\n---\n\n\n\n\n# Overview\n\nThis is an extension of [a `cmdstanr`/Stan model](/posts/2024-02-15-longitudinal-multilevel-bayes-5/) to fit longitudinal data using Bayesian multilevel/hierarchical/mixed-effects models. It is a continuation of a prior series of posts. You should [start at the beginning](/posts/2022-02-22-longitudinal-multilevel-bayes-1/). \n\nHere is [the Stan code for this example](stancode-4par.stan) and this is [the R script that runs everything](cmdstanr-4par-script.R).\n\n\n# Introduction\n\nAs described [in the prior post](/posts/2024-02-15-longitudinal-multilevel-bayes-5/) I want to implement an ordinary differential equation (ODE) model with Stan. I am slowly building up to that. \n\nWhile I had planned to hop from the 2-parameter `cmdstanr` and Stan model straight to the ODE model, I ended up deciding on one more intermediate step. Namely, I know that the ODE model I want to use has 4 main parameters, as opposed to the 2 parameters used here. I figured I might first build a more complex, 4-parameter non-ODE model before I switch. The obvious candidate for that 4-parameter model is the equation I [mentioned previously](/posts/2022-02-25-longitudinal-multilevel-bayes-4/#alternative-model-for-time-series-trajectory) and that we ended up using in [one of our  papers](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10310361/).\n\n\n\n# Model setup\n\n[The prior post](/posts/2024-02-15-longitudinal-multilevel-bayes-5/) revisited the model briefly. The only part that's different here is that I'm using a different main model to describe the virus load trajectories. This model has more parameters, therefore there are more equations. [I briefly described the model I will use here](/posts/2022-02-25-longitudinal-multilevel-bayes-4/#alternative-model-for-time-series-trajectory). A somewhat more detailed description is provided [in one of our papers](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10310361/), or the [original paper that came up with it](https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-11-S1-S10).\n\nI'm giving the parameters different names ($\\alpha$, $\\beta$, $\\gamma$, $\\eta$ instead of $p$, $g$, $d$, $k$) to be consistent with what I've been doing so far. Otherwise it's the same model described in that previous post. Of course, each of the new parameters needs to be further specified and assigned distributions. Here are the components of the the new  model.\n\n\n## Main model structure\n\nThis defines the main trajectory equation, which is the new part of the model.\n\n\n$$\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{main model describing the virus trajectory} \\\\\n\\mu_{i,t} = \\log\\left( \\frac{2 \\exp(\\alpha_{i})}{e^{-\\exp(\\beta_{i})  (\\exp(\\gamma_{i}) - t_i)} + e^{\\exp(\\eta_{i})  (t_i - \\exp(\\gamma_{i}))}}\\right).\\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\gamma_{i}   =  g_{0,i} + g_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\eta_{i}   =  e_{0,i} + e_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\end{aligned}\n$$\n\nNote that the original model was developed and set up with the assumptions that all 4 main parameters are positive. \nTo ensure that, I'll use the [previously described approach](/posts/2022-02-22-longitudinal-multilevel-bayes-1/#numerical-trickeries) of exponentiating the parameters. \n\n\n\n## Parameter distributions\n\nTo fully specify the model, we need to give all parameters distributions. \nHere are the distributions for the population-level parameters. These do not vary between individuals. \n\nI already know that my model is too flexible/complex for the data, since the data was generated using the 2-parameter model and I'm fitting it with 4. \n\nThis overfitting might lead to poor performance of the fitting routine. To minimize potential problems, I'll set priors somewhat narrow. That's generally not the best idea, you don't want your priors to be overly influential, unless you have very strong _a priori_ scientific knowledge why they should be very constraining.\n\n\n$$\n\\begin{aligned}\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{Exponential}(1)  \\\\\na_1 \\sim \\mathrm{Normal}(0, 0.1) \\\\\nb_1 \\sim \\mathrm{Normal}(0, 0.1) \\\\\ng_1 \\sim \\mathrm{Normal}(0, 0.1) \\\\\ne_1 \\sim \\mathrm{Normal}(0, 0.1) \\\\\n\\end{aligned}\n$$\n\n\nIn addition, we allow some parameters to differ between individuals, and we'll implement hyper-parameters to allow these values to inform each other across individuals. This is again the adaptive pooling concept discussed previously.\n\nI'm setting values for the prior distributions similar to values we used in our [previous study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10310361/). That's probably not ideal since the data is different, but hopefully close enough that the code will run.\n\n\n$$\n\\begin{aligned}\n\\textrm{individal-level priors} \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_b) \\\\\ng_{0,i} \\sim \\mathrm{Normal}(\\mu_g, \\sigma_g) \\\\\ne_{0,i}  \\sim \\mathrm{Normal}(\\mu_e, \\sigma_e) \\\\\n\\\\\n\\textrm{hyper priors} \\\\\n\\mu_a  \\sim \\mathrm{Normal}(25, 5) \\\\\n\\mu_b  \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_g  \\sim \\mathrm{Normal}(0, 1) \\\\\n\\mu_e  \\sim \\mathrm{Normal}(-1, 0.5) \\\\\n\\sigma_a  \\sim \\mathrm{Exponential}(1)  \\\\\n\\sigma_b  \\sim \\mathrm{Exponential}(1)  \\\\\n\\sigma_g  \\sim \\mathrm{Exponential}(1)  \\\\\n\\sigma_e  \\sim \\mathrm{Exponential}(1)  \\\\\n\\end{aligned}\n$$\n\n\n\nAnd that's the full model. The basic structure is the same as before, but the model is bigger because I'm now modeling the virus trajectory (given by $\\mu_{i,t}$) with 4 main parameters.\n\n\n# Model implementation\n\nWe'll follow exactly the same setup as [in the previous post](/posts/2024-02-15-longitudinal-multilevel-bayes-5/).\nLinks to the Stan and R code files are given at the top of this document.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('here') #for file loading\nlibrary('dplyr') # for data manipulation\nlibrary('ggplot2') # for plotting\nlibrary('fs') #for file path\nlibrary('cmdstanr') #for model fitting\nlibrary('bayesplot') #for plotting results\nlibrary('loo') #for model diagnostics\n```\n:::\n\n\n\nWe'll use the same data as before. I'm making one more change. Instead of hard-coding the values for the prior distributions into the Stan code, I'm assigning some of them labels and then pass values into Stan from R. This makes exploring the Stan model more flexible, I don't need to re-edit the Stan code if I want to try different values for the priors. These values for the priors need to be passed in as part of the data. I could do this for all parameters, but out of laziness, I'm only doing it for the individual-level hyper-parameters. Of course one could do it for all parameters, and I probably would do it for a real research problem where I expect having to fiddle with the priors a bit, but the main point here is to illustrate this approach, so I'm keeping it simple.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adjust as necessary\nsimdatloc <- here::here('posts','2022-02-22-longitudinal-multilevel-bayes-1','simdat.Rds')\nsimdat <- readRDS(simdatloc)\n# using dataset 3 for fitting\n# also removing anything in the dataframe that's not used for fitting\n# makes the fitting more robust\n# We format the data slightly differently compared to the prior examples\n# We only store the dose for each individual\n# And also add number of observations and individuals\nNind = length(unique(simdat$m3$id))\nNobs =  length(simdat$m3$id)\n# values for prior distributions\n# allows for exploring different values without having to edit Stan model code\npriorvals = list(mu_a_mu = 7, mu_a_sd = 3,\n                 mu_b_mu = 1, mu_b_sd = 1,\n                 mu_g_mu = 3, mu_g_sd = 1,\n                 mu_e_mu = -3, mu_e_sd = 1\n)\n\n# all data as one list, this is how Stan needs it\nfitdat=list(id=simdat[[3]]$id,\n            outcome = simdat[[3]]$outcome,\n            time = simdat[[3]]$time,\n            dose_adj = simdat[[3]]$dose_adj[1:Nind], #first Nind values\n            Nobs =  Nobs,\n            Nind = Nind\n            )\nfitdat = c(fitdat,priorvals)\n```\n:::\n\n\n\n# Stan code\n\nNext, we need to write the Stan model code. While one could embed Stan code inside an R script, I find it best to write that code in a separate file and then load it. Here, the code is in file called `stancode-4par.stan`. This code loads and compiles the Stan model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make Stan model. \nstanmod1 <- cmdstanr::cmdstan_model(here('posts','2024-02-16-longitudinal-multilevel-bayes-6',\"stancode-4par.stan\"), \n                                    pedantic=TRUE, \n                                    force_recompile=TRUE)\n```\n:::\n\n\nNext, we'll take a quick look at the Stan model code. The model is basically like the previous one, updated to reflect the model equations above. Another change I made is instead of hard-coding values for prior distributions inside the Stan code, I'm passing some of them into the code as data. I could (and probably should have) passed them all in, but I got lazy üòÅ. The advantage of passing them in is that I can more quickly play around with different values and see how results change. It also ensures that I use the same values in all parts of the model (e.g., `model` and `generated quantities` blocks).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(stanmod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n//\n// Stan code for fitting a hierarchical model to time-series data\n//\n\ndata{\n   int<lower = 1> Nobs; //number of observations\n   int<lower = 1> Nind; //number of individuals\n   vector[Nobs] outcome; //virus load\n   vector[Nobs] time; // times at which virus load is measured\n   vector[Nind] dose_adj; //dose after adjustment, 1 value per individual\n   array[Nobs] int id;  //vector of person IDs to keep track which data points belong to whom\n   //everything below are variables that contain values for prior distributions\n   real mu_a_mu; \n   real mu_b_mu;\n   real mu_g_mu;\n   real mu_e_mu;\n   real mu_a_sd;\n   real mu_b_sd;\n   real mu_g_sd;\n   real mu_e_sd;\n}\n\nparameters{\n    // population variance\n    real<lower=0> sigma;\n    // variance of priors\n    real<lower=0> sigma_a;\n    real<lower=0> sigma_b;\n    real<lower=0> sigma_g;\n    real<lower=0> sigma_e;\n    // population-level dose dependence parameters\n    real a1;\n    real b1;\n    real g1;\n    real e1;\n    // hyper-parameters to implement adaptive pooling\n    real mu_a;\n    real mu_b;\n    real mu_g;\n    real mu_e;\n    // individual level variation parameters\n    vector[Nind] a0;\n    vector[Nind] b0;\n    vector[Nind] g0;\n    vector[Nind] e0;\n}\n\n\n// Generated/intermediate parameters\ntransformed parameters{\n\n    // predicted virus load from model\n    vector[Nobs] virus_pred; \n    // main model parameters\n    // each individual has their potentially own value \n    vector[Nind] alpha;\n    vector[Nind] beta;\n    vector[Nind] gamma;\n    vector[Nind] eta;\n\n    // compute main model parameters\n    for ( i in 1:Nind ) {\n        alpha[i] = a0[i] + a1 * dose_adj[i];\n        beta[i] = b0[i] + b1 * dose_adj[i];\n        gamma[i] = g0[i] + g1 * dose_adj[i];\n        eta[i] = e0[i] + e1 * dose_adj[i];\n        \n    }\n    // loop over all observations\n    // since parameters are saved in vectors of length corresponding to number of individuals\n    // we need to index with that extra id[i] notation\n    for (i in 1:Nobs)\n    {\n      virus_pred[i] = log( 2*exp(alpha[id[i]]) / ( exp( -exp(beta[id[i]]) * (exp(gamma[id[i]]) - time[i]) )  +  exp( exp(eta[id[i]]) * (time[i] - exp(gamma[id[i]])) )  ) ) ;\n     }\n\n} // end transformed parameters block\n\n\n\nmodel{\n\n    // residual population variation\n    sigma ~ exponential( 0.1 ); \n    // variance of priors\n    sigma_a ~ exponential( 1 );\n    sigma_b ~ exponential( 1 );\n    sigma_g ~ exponential( 1 );\n    sigma_e ~ exponential( 1 );\n    // average dose-dependence of each ODE model parameter\n    a1 ~ normal( 0 , 0.2); \n    b1 ~ normal( 0 , 0.2);\n    g1 ~ normal( 0.5, 0.2 );\n    e1 ~ normal( 0 , 0.2 );\n    // hyper-priors to allow for adaptive pooling among individuals \n    // values for the distributions are passed into the Stan code as part of the data\n    mu_a ~ normal( mu_a_mu , mu_a_sd );\n    mu_b ~ normal( mu_b_mu , mu_b_sd );\n    mu_g ~ normal( mu_g_mu , mu_g_sd );\n    mu_e ~ normal( mu_e_mu , mu_e_sd );\n    // individual variation of each ODE model parameter\n    a0 ~ normal( mu_a , sigma_a );\n    b0 ~ normal( mu_b , sigma_b );\n    g0 ~ normal( mu_g , sigma_g );\n    e0 ~ normal( mu_e , sigma_e );\n\n    // distribution of outcome (virus load)\n    // all computations to get the time-series trajectory for the outcome are done  \n    // inside the transformed parameters block\n    outcome ~ normal( virus_pred , sigma );\n}\n\n// for model diagnostics and exploration\ngenerated quantities {\n    // define quantities that are computed in this block\n    vector[Nobs] ypred;\n    vector[Nobs] log_lik;\n    real<lower=0> sigma_prior;\n    real<lower=0> sigma_a_prior;\n    real<lower=0> sigma_b_prior;\n    real<lower=0> sigma_g_prior;\n    real<lower=0> sigma_e_prior;\n    real a1_prior;\n    real b1_prior;\n    real g1_prior;\n    real e1_prior;\n    real mu_a_prior;\n    real mu_b_prior;\n    real mu_g_prior;\n    real mu_e_prior;\n    real a0_prior;     // same prior for each individual so only specify one\n    real b0_prior;     \n    real g0_prior;     \n    real e0_prior;     \n    \n    \n    // this is so one can plot priors and compare with posterior later   \n    // simulate the priors\n    sigma_prior = exponential_rng( 0.1 );\n    sigma_a_prior =  exponential_rng(  1 );\n    sigma_b_prior = exponential_rng(  1 );\n    sigma_g_prior = exponential_rng(  1 );\n    sigma_e_prior = exponential_rng(  1 );\n    a1_prior = normal_rng( 0 , 0.2);\n    b1_prior = normal_rng( 0 , 0.2);\n    g1_prior = normal_rng( 0.5 , 0.2);\n    e1_prior = normal_rng( 0 , 0.2);\n    mu_a_prior = normal_rng( mu_a_mu , mu_a_sd );\n    mu_b_prior = normal_rng( mu_b_mu , mu_b_sd);\n    mu_g_prior = normal_rng( mu_g_mu , mu_g_sd);\n    mu_e_prior = normal_rng( mu_e_mu , mu_e_sd);\n  \n    a0_prior = normal_rng(mu_a, sigma_a);\n    b0_prior = normal_rng(mu_b, sigma_b);\n    g0_prior = normal_rng(mu_g, sigma_g);\n    e0_prior = normal_rng(mu_e, sigma_e);\n  \n  // compute log-likelihood and predictions\n    for(i in 1:Nobs)\n    {\n      log_lik[i] = normal_lpdf(outcome[i] | virus_pred[i], sigma);\n      ypred[i] = normal_rng(virus_pred[i], sigma);\n    }\n} //end generated quantities block \n```\n\n\n:::\n:::\n\n\n\n\n\n## Model fitting settings\n\nThese are the settings for the model fitting routine. Basically the same as before, only more initial conditions now because we have more parameters. And of course different values, since our model changed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#settings for fitting\nfs_m1 = list(warmup = 1500,\n             sampling = 1000, \n             max_td = 15, #tree depth\n             adapt_delta = 0.999,\n             chains = 5,\n             cores  = 5,\n             seed = 1234,\n             save_warmup = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# separate definition of initial values, added to fs_m1 structure \n# a different sample will be drawn for each chain\n# there's probably a better way to do that than a for loop\nset.seed(1234) #make inits reproducible\ninit_vals_1chain <- function() (list(mu_a = runif(1,5,10), \n                                     mu_b = runif(1,1,2),\n                                     mu_g = runif(1,1,4),\n                                     mu_e = runif(1,-4,-2),\n                                     sigma_a = runif(1,0,2),\n                                     sigma_b = runif(1,0,2),\n                                     sigma_g = runif(1,0,2),\n                                     sigma_e = runif(1,0,2),\n                                     a1 = rnorm(1,-0.1,0.1),\n                                     b1 = rnorm(1,-0.1,0.1),\n                                     g1 = rnorm(1,-0.1,0.1),\n                                     e1 = rnorm(1,-0.1,0.1),\n                                     sigma = runif(1,0,2)))\ninits = NULL\nfor (n in 1:fs_m1$chains)\n{\n  inits[[n]] = init_vals_1chain()\n}\nfs_m1$init = inits\n```\n:::\n\n\n# Model fitting \n\nAt this point, we have specified everything necessary to run the model. This runs the model with the specified settings. I'm supressing the output but it's useful to look at it when you run it to make sure the sampler is running ok.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_m1 <- stanmod1$sample(data = fitdat,\n                          chains = fs_m1$chains,\n                          init = fs_m1$init,\n                          seed = fs_m1$seed,\n                          parallel_chains = fs_m1$chains,\n                          iter_warmup = fs_m1$warmup,\n                          iter_sampling = fs_m1$sampling,\n                          save_warmup = fs_m1$save_warmup,\n                          max_treedepth = fs_m1$max_td,\n                          adapt_delta = fs_m1$adapt_delta\n)\n```\n:::\n\n\n\n# Model exploration\n\nFirst, we look at diagnostics from the fitting routine to make sure nothing obviously wrong shows up.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_m1$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProcessing csv files: C:/Users/Andreas/AppData/Local/Temp/Rtmp2t2Ct5/stancode-4par-202402131131-1-3219cb.csvWarning: non-fatal error reading adaptation data\n, C:/Users/Andreas/AppData/Local/Temp/Rtmp2t2Ct5/stancode-4par-202402131131-2-3219cb.csvWarning: non-fatal error reading adaptation data\n, C:/Users/Andreas/AppData/Local/Temp/Rtmp2t2Ct5/stancode-4par-202402131131-3-3219cb.csvWarning: non-fatal error reading adaptation data\n, C:/Users/Andreas/AppData/Local/Temp/Rtmp2t2Ct5/stancode-4par-202402131131-4-3219cb.csvWarning: non-fatal error reading adaptation data\n, C:/Users/Andreas/AppData/Local/Temp/Rtmp2t2Ct5/stancode-4par-202402131131-5-3219cb.csvWarning: non-fatal error reading adaptation data\n\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\n25 of 12500 (0.20%) transitions ended with a divergence.\nThese divergent transitions indicate that HMC is not fully able to explore the posterior distribution.\nTry increasing adapt delta closer to 1.\nIf this doesn't remove all divergences, try to reparameterize the model.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nThe E-BFMI, 0.06, is below the nominal threshold of 0.30 which suggests that HMC may have trouble exploring the target distribution.\nIf possible, try to reparameterize the model.\n\nEffective sample size satisfactory.\n\nThe following parameters had split R-hat greater than 1.05:\n  sigma_a, sigma_g, sigma_e\nSuch high values indicate incomplete mixing and biased estimation.\nYou should consider regularizating your model with additional prior information or a more effective parameterization.\n\nProcessing complete.\n```\n\n\n:::\n:::\n\n\nOk, so the sampler isn't quite happy. We should sample more and more stringently, but that would take very long. So for the purpose of this investigation, and given that I'm only exploring this model as a stepping stone to the ODE model I'm really interested in, I'll leave it the way it is. If this were an actual research project, I would obviously need to improve the model performance.\n\nNext, we'll take a look at a summary table for the distributions of some of the model parameters. I cut it off since there are too many to show (each individual as multiple parameters with associated distributions).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# uses posterior package \nprint(head(res_m1$summary(),15))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 √ó 10\n   variable      mean    median      sd     mad       q5      q95  rhat ess_bulk\n   <chr>        <dbl>     <dbl>   <dbl>   <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n 1 lp__     -844.     -847.     42.1    41.3    -9.08e+2 -768.     1.07     78.4\n 2 sigma      21.4      21.4     0.968   0.952   1.99e+1   23.1    1.00   2541. \n 3 sigma_a     0.728     0.461   0.752   0.417   8.11e-2    2.34   1.09     47.5\n 4 sigma_b     0.166     0.156   0.0958  0.0929  2.71e-2    0.341  1.04    113. \n 5 sigma_g     0.165     0.139   0.128   0.118   2.02e-2    0.410  1.13     28.7\n 6 sigma_e     0.362     0.255   0.324   0.235   4.55e-2    1.03   1.14     32.6\n 7 a1          0.124     0.126   0.202   0.203  -2.14e-1    0.457  1.00   3487. \n 8 b1         -0.0215   -0.0210  0.103   0.104  -1.90e-1    0.149  1.02    384. \n 9 g1          0.521     0.514   0.126   0.122   3.26e-1    0.743  1.00   1788. \n10 e1          0.0997    0.0998  0.186   0.182  -2.09e-1    0.410  1.00   1818. \n11 mu_a        5.54      5.61    1.86    1.94    2.52e+0    8.48   1.02    310. \n12 mu_b        0.800     0.801   0.232   0.235   4.19e-1    1.18   1.03    291. \n13 mu_g        3.08      3.10    0.177   0.170   2.76e+0    3.34   1.02    327. \n14 mu_e       -3.20     -3.13    0.699   0.669  -4.47e+0   -2.19   1.03    238. \n15 a0[1]       5.55      5.57    2.09    2.08    2.12e+0    8.84   1.01    378. \n# ‚Ñπ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\nWe can't really compare with the values we used to generate the model since we are using a different model to fit the data, so we shouldn't expect any parameters to be similar. So we need further diagnostics to decide if the model did a reasonable job.\n\n\n\nSo let's again look at a few plots to make sure everything looks reasonable. We'll first need to get the samples, both with and without warmups, to be able to make various figures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#this uses the posterior package to get draws\nsamp_m1 <- res_m1$draws(inc_warmup = FALSE, format = \"draws_df\")\nallsamp_m1 <- res_m1$draws(inc_warmup = TRUE, format = \"draws_df\")\n```\n:::\n\n\n\nNow we can look at a few figures. Here I'm showing again a trace plot, a posterior density plot and a pairs plot. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# only main parameters, excluding parameters that we have for each individual, is too much\nplotpars = c(\"a1\",\"b1\",\"g1\",\"e1\",\"sigma\")\nbayesplot::color_scheme_set(\"viridis\")\nbp1 <- bayesplot::mcmc_trace(samp_m1, pars = plotpars)\nbp2 <- bayesplot::mcmc_dens_overlay(samp_m1, pars = plotpars)\nbp3 <- bayesplot::mcmc_pairs(samp_m1, pars = plotpars)\nplot(bp1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_par_m1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(bp2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_par_m1-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(bp3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_par_m1-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# just to get a picture that can be shown together with the post\nggsave(\"featured.png\",bp1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSaving 7 x 5 in image\n```\n\n\n:::\n:::\n\n\n\n\nNext, we'll compare prior and posterior distributions. This can give an indication if the priors were selected well or are too broad or overly influential. To be able to show priors, we needed all that extra information in the _generated quantities_ block in the Stan code.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data manipulation to get in shape for plotting\npostdf <- samp_m1 %>% select(!ends_with('prior')) %>% select(!starts_with(\".\")) %>% select(-\"lp__\") %>% select(!contains(\"[\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n```{.r .cell-code}\npriordf <- samp_m1 %>% select(ends_with('prior')) %>% rename_with(~ gsub(\"_prior\", \"\", .x, fixed = TRUE) ) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n```{.r .cell-code}\npostlong <- tidyr::pivot_longer(data = postdf, cols = everything() , names_to = \"parname\", values_to = \"value\") %>% mutate(type = \"posterior\")\npriorlong <- tidyr::pivot_longer(data = priordf, cols = everything() , names_to = \"parname\", values_to = \"value\") %>% mutate(type = \"prior\")\nppdf <- dplyr::bind_rows(postlong,priorlong)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1_p1 <- ppdf %>%\n  ggplot() +\n  geom_density(aes(x = value, color = type), linewidth = 1) +\n  facet_wrap(\"parname\", scales = \"free\") +\n  theme_minimal()\nplot(m1_p1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/prior_post_m1-1.png){width=672}\n:::\n:::\n\n\nThe plots show that the priors don't overly influence the results, the data dominates the posteriors (they move and get more peaked, an indication that the data controls the posterior shape).\n\nAnother useful plot is to look at observed versus predicted results. This is shown in the following plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred_df <- samp_m1 %>% select(starts_with(\"ypred\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n```{.r .cell-code}\nm1_p2 <- bayesplot::ppc_dens_overlay(fitdat$outcome, as.matrix(ypred_df))\nplot(m1_p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/obs_pred_m1-1.png){width=672}\n:::\n:::\n\n\n\nThe data (black line, $y$ variable) and the model (thin green line, $y_{rep}$) are following each other fairly closely. That's a good sign. Systematic deviations would indicate that the model didn't fully capture the patterns found in the data and might need modification.\n\n\nHere's again some further exploration via cross-validation with the `loo` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# uses loo package \nloo_m1 <- res_m1$loo(cores = fs_m1$chains, save_psis = TRUE)\nprint(loo_m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 5000 by 264 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -1189.8 18.5\np_loo         9.8  1.6\nlooic      2379.6 37.0\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     257   97.3%   177       \n (0.5, 0.7]   (ok)         6    2.3%   244       \n   (0.7, 1]   (bad)        1    0.4%   2028      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \nSee help('pareto-k-diagnostic') for details.\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(loo_m1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/loo_m1_part1-1.png){width=672}\n:::\n:::\n\n\nSome values aren't too great. But it's only a few and it's very likely that if we were to run the model with more iterations and more stringent settings, we'll get even better results. You can give it a try üòÅ. \n\nHere's some more LOO diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred_df <- samp_m1 %>% select(starts_with(\"ypred\"))\nm1_p3 <- bayesplot::ppc_loo_pit_overlay(\n  y = fitdat$outcome,\n  yrep = as.matrix(ypred_df),\n  lw = weights(loo_m1$psis_object)\n)\nplot(m1_p3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/loo_m1_part2-1.png){width=672}\n:::\n:::\n\n\nThe marginal posterior predictive plot suggests some improvement might be possible (so that the solid line is more on top of the green lines). [See here for more](https://mc-stan.org/loo/articles/loo2-example.html).\n\n\n\n# Summary and continuation\n\nThis completes the 4-parameter model. I just wanted to get a working model, I'm not really interested in the model results. I just wanted to set the stage for the next version, which is the 4-parameter ODE model. So it's finally time to [tackle that one](/posts/2024-02-17-longitudinal-multilevel-bayes-7/).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}