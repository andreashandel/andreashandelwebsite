---
title: Bayesian analysis of longitudinal multilevel data - part 5  
description: Part 5 of a tutorial showing how to fit directly with Stan and cmdstanr.
author: Andreas Handel
date: 2024-02-15
date-modified: 2025-03-05
aliases: 
  - ../longitudinal-multilevel-bayesian-analysis-5/
categories: 
  - R
  - Data Analysis
  - Bayesian
  - Stan
image: "featured.png"
image-alt: "Density plot of priors and posteriors for several model parameters."
execute:
  echo: true
engine: knitr
---



```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('cmdstanr-2par-script.R')
```

# Overview

This is a re-implementation of a prior model using `cmdstanr` and Stan code. It is a continuation of a prior series of posts. You should [start at the beginning](/posts/2022-02-22-longitudinal-multilevel-bayes-1/). 

Here is [the Stan code for this example](stancode-2par.stan) and this is [the R script that runs everything](cmdstanr-2par-script.R).


# Introduction

A while ago, I wrote [a series of tutorials](/posts/2022-02-22-longitudinal-multilevel-bayes-1/) that discuss fitting longitudinal data using Bayesian multilevel/hierarchical/mixed-effects models.

For a research project, I now want to implement a model that uses a set of ordinary differential equations (ODEs). I figured to understand what I'm trying to do, I should first teach myself and write it up in a tutorial. 

To implement ODEs with Stan, one can't fully use the `rethinking` or `brms` package, one needs to write at least some Stan code. Based on my needs, it is best if I fully implement the model in Stan and call it from R through `cmdstanr`.

I was going to do all at once, but then realized it's better if I first re-implement the old (non ODE-based) setup with Stan code, and then once that's up and running, switch to the ODE model. 

So this post is an intermediary step to my final goal. It might be of interest to folks to see how to implement this question fully with Stan, even if they don't plan on using ODEs.


# Quick recap

I assume you read through the previous posts, at least [part 1](/posts/2022-02-22-longitudinal-multilevel-bayes-1/) which describes the overall setup and the models to be explored, and [part 2](/posts/2022-02-23-longitudinal-multilevel-bayes-2/) which explains the models further and fits the data using `rethinking`. If you didn't, the following won't make much sense üòÅ.

Previously, I explored several model variants. Here, I'm focusing on the adaptive pooling model (which I previously labeled model 4). As a repeat, here are the model equations.
I changed the distributions for a few of the parameters since it turned out the ones I used previously made it tricky to sample the priors with Stan and led to convergence issues.


$$
\begin{aligned}
\textrm{Outcome} \\
Y_{i,t}   \sim \mathrm{Normal}\left(\mu_{i,t}, \sigma\right) \\
\\
\textrm{main model describing the virus trajectory} \\
\mu_{i,t}   =  \exp(\alpha_{i}) \log (t_{i}) -\exp(\beta_{i}) t_{i} \\
\\
\textrm{Deterministic models for main parameters} \\
\alpha_{i}   =  a_{0,i} + a_1 \left(\log (D_i) - \log (D_m)\right)  \\
\beta_{i}   =  b_{0,i} + b_1 \left(\log (D_i) - \log (D_m)\right) \\
\\
\textrm{population-level priors} \\
\sigma  \sim \mathrm{Exponential}(1)  \\
a_1 \sim \mathrm{Normal}(0.1, 0.1) \\
b_1 \sim \mathrm{Normal}(-0.1, 0.1) \\
\\
\textrm{individal-level priors} \\
a_{0,i} \sim \mathrm{Normal}(\mu_a, \sigma_a) \\
b_{0,i}  \sim \mathrm{Normal}(\mu_b, \sigma_b) \\
\\
\textrm{hyper priors} \\
\mu_a  \sim \mathrm{Normal}(3, 1) \\
\mu_b  \sim \mathrm{Normal}(0.5, 1) \\
\sigma_a  \sim \mathrm{Exponential}(1)  \\
\sigma_b  \sim \mathrm{Exponential}(1)  
\end{aligned}
$$


# Model implementation

I previously used the `brms` and `rethinking` R packages to run our `Stan` models in `R`, without having to write `Stan` code. Of course, we could implement the model above in either of those packages. But in preparation of what I really want to do (using ODE models, and eventually fully account for censored data), I need to switch to coding the model in `Stan`. There might be hacks to do it with `brms` or `rethinking`, but it seems more flexible and also more transparent to just code the full model in `Stan`. We'll still run it through R using `cmdstanr`. Links to the Stan and R code files are given at the top of this document.

We start by loading packages and defining a few settings.

```{r, packages, message=FALSE, warning = FALSE}
```

```{r, setup, message=FALSE, warning = FALSE}
```


We'll use the same data as before. We just need to reshape it a bit to get it into the format that `cmdstanr` requires.


```{r, data}
```


# Stan code

We need to write the Stan model code. While one could embed Stan code inside an R script, I find it best to write that code in a separate file and then load it. Here, the code is in file called `stancode-2par.stan`. This is how it looks like. 

```{.stan include="stancode-2par.stan"}
```

I added some comments to the Stan model, but if you have never written Stan code, this is likely not fully clear. I won't try to explain Stan code in detail here. There are lots of good resources on the [Stan website](https://mc-stan.org/) and other places online. 
Note that the `generated quantities` block is not technically part of the model, and we would get the same results without it. But it is included to compute priors, likelihood and posterior predictions, so we can explore those later. You'll see that used below.
While we could have all the information that's inside the `transformed parameters` block part of the `model` block, that would mean that the `generated quantities` code bits can't access that information. Therefore, all of these intermediate steps are done in `transformed parameters` and then used in both the `model` and the `generated quantities` block. I generally find it easiest to have any distributional/probabilistic bits of code (those that include the `~` sign) in the `model` block, and everything else as much as possible in `transformed parameters`.


This uses `cmdstanr` to load and compile the Stan model.

```{r, make_stanmodel, message=FALSE, warning = FALSE, eval=FALSE}
```



# Model fitting settings

To fully specify a model, we need to define the details of the model run (e.g., the random seed, the number of warm-up and sampling steps). This often requires some tuning to find the right values. It is generally a good idea to start with fewer iterations and less stringent fitting criteria while debugging the code. Once everything seems working, one can do one long final run. 


```{r, fitconditions}
```


It is also a good idea to specify starting values for the parameters. Models can be run without providing starting values (also called initial conditions). In that case, `cmdstanr` will pick default values. However, setting starting values can often improve convergence and thus cut down on required computing time. It also requires one to think a bit more carefully about their model, which is a good thing üòÅ.


```{r, initialconditions}
```


# Model fitting 

At this point, we have specified everything necessary to run the model. This runs the model with the specified settings. I'm supressing the output but it's useful to look at it when you run it to make sure the sampler is running ok.

This runs the model. It's not actually run here to speed up generation of this Quarto file, but the code chunk is in the R script, so you can run it.

```{r, run_m1, message=FALSE, warning = FALSE, output = FALSE, eval=FALSE}
```


# Model result loading

To save time, we don't run the model each time, instead we save the results and load them.

```{r, loadfits, message=FALSE, warning = FALSE}
```



# Model diagnostics

First, we look at diagnostics from the fitting routine to make sure nothing obviously wrong shows up.


```{r, diagnose_m1,  warning = FALSE, message = FALSE}
```

Things look reasonably good, no obvious problems.


Another important check are to make a few diagnostic plots. We'll first need to get the samples, both with and without warmups, to be able to make various figures.

```{r, get_samples_m1,  warning = FALSE}
```

Now we can look at a few figures. Here I'm showing a trace plot  and a pairs plot. I'm not discussing the plots in detail, you can look up the help file for each R command to learn more.

Note that I'm including some priors here. That's not too meaningful since the prior distributions do not change as the fitting proceeds. Still, it won't hurt to see them. If for some reason the trace plots for the priors look strange (e.g., indicating poor mixing), it means something in the code is wrong.  Similarly, none of the priors should show correlations with each other in the pairs plot.

```{r, plot_par_m1,  warning = FALSE}
```

The plots look reasonable. Well-mixing chains and no noticeable correlations among parameters.

# Model results

Now that we think we can somewhat trust that the sampling worked, we'll take a look at a summary table for the distributions of some of the model parameters. I cut it off since there are too many to show (each individual as multiple parameters with associated distributions). We'll also look at the posteriors in a graph.

```{r, results_m1}
```


The estimates for the parameters are fairly close to those used to simulate the data and obtained [previously](/posts/2022-02-23-longitudinal-multilevel-bayes-2/#models-4-and-4a). That's reassuring, since we just re-fit the same model[^priors] using a different R package, we really want to get the same results. 

[^priors]: We'll, it's almost the same model, as I mentioned above some of the prior distributions got changed, but that did fortunately not impact our posterior estimates by much.



# Priors and Posteriors

Next, we'll compare prior and posterior distributions. This can give an indication if the priors were selected well or are too broad or overly influential. To be able to show priors, we needed all that extra information in the _generated quantities_ block in the Stan code. 
The individual-level posteriors for $a_0[i]$ and $b_0[i]$ are omitted since there are too many. With a bit more data wrangling, one could plot the averages for these parameters, but I couldn't quickly think of how to do it, so I'm skipping it üòÅ. The priors for these parameters are shown since they same for each individual. 

```{r, prep_data_m1,  warning = FALSE}
```


```{r, prior_post_m1,  warning = FALSE}
```

The plots show that the priors don't overly influence the results, the data dominates the posteriors (they move and get more peaked, an indication that the data controls the posterior shape).


# Observed versus predicted

Another useful plot is to look at observed versus predicted results. This is shown in the following plot. The data (black line, $y$ variable) and the model (thin green line, $y_{rep}$) are following each other fairly closely. That's a good sign. Systematic deviations would indicate that the model didn't fully capture the patterns found in the data and might need modification.


```{r, obs_pred_m1, cache=TRUE}
```


# Cross-validation tests

We can explore further doing cross-validation with the `loo` package. For this to work, the Stan code needs to include computation of the log-likelihood (stored in a variable called `log_lik`). We included that in the `Stan` code for this model.

Here are the diagnostics we get from `loo`. I won't go into the details of cross-validation and LOO here, see the [`loo` package website](https://mc-stan.org/loo/) for good explanations.

```{r, loo_m1_part1,  warning = FALSE, message = FALSE, cache=TRUE}
```

Some values aren't too great. But it's only a few and it's very likely that if we were to run the model with more iterations and more stringent settings, we'll get even better results. You can give it a try üòÅ. 

Here's some more LOO diagnostics.

```{r, loo_m1_part2,  warning = FALSE, message = FALSE, cache=TRUE}
```

The marginal posterior predictive plot suggests some improvement might be possible (so that the solid line is more on top of the green lines). [See here for more](https://mc-stan.org/loo/articles/loo2-example.html).


# Model predictions

Finally, we want to look at the actual data and the model predictions. 
These lines of code compute the predictions for both the deterministic part of the model only (the mean) and for individual predictions that account for additional variability around the mean.

```{r, make_predictions,  warning = FALSE, message = FALSE}
```

This plots the data and the predictions, similarly to previous posts of this series. 
As before, it shows the deterministic mean and credible interval (I chose 95% here) and the prediction intervals (the very light shaded areas, also chosen here at 95%). Agreement of model with data is good, as before.

Note that previously, I used new data to predict at more time points to get smoother curves. That was easy to do with `rethinking` and `cmdstanr`. I couldn't figure out an easy way to do it here. It is possible to re-run a version of the Stan model that only includes the `generated quantities` block and give it new "data" with more time points and thereby return predictions. It should also be possible to do this with R code by recomputing trajectories for more data given the parameter distributions. I couldn't right away think of a good way of doing it, so left it as is. One can still reasonably well compare models and data.

```{r, plot_predictions,  warning = FALSE, message = FALSE}
```



# Summary and continuation

This completes the `cmdstanr`/`Stan` re-implementation and exploration of one of the previously explored models. Comparing the results to those found previously, we find good agreement. That's somewhat comforting.

While I had planned to now implement the ODE model as a next step, I ended up deciding on one more intermediate step. Namely, I know that the ODE model I want to use has 4 main parameters, as opposed to the 2 parameters used here. I figured I might first build a more complex, 4-parameter non-ODE model before I switch. The obvious candidate for that 4-parameter model is the equation I [mentioned previously](/posts/2022-02-25-longitudinal-multilevel-bayes-4/#alternative-model-for-time-series-trajectory) and that we ended up using in [our paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10310361/).

So [the next part in this series](/posts/2024-02-16-longitudinal-multilevel-bayes-6/) is a more complex non-ODE model. 

<!-- I recommend you go there and at least skim through it. Nothing much new is happening, but it sets the stage for what's coming after. Or, if you are impatient, [jump straight to the ODE model implementation](/posts/2024-02-17-longitudinal-multilevel-bayes-7/). -->









